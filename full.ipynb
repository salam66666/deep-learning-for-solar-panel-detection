{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127df17-1213-4f13-8346-2270f6bd28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 路径设置\n",
    "train_image_dir = r'E:\\CDUT\\English\\7 Term\\Project\\model\\archive\\dataset\\images'\n",
    "train_annotation_dir = r'E:\\CDUT\\English\\7 Term\\Project\\model\\archive\\dataset\\annotations'\n",
    "\n",
    "val_image_dir = r'E:\\CDUT\\English\\7 Term\\Project\\model\\archive\\dataset_2\\images'\n",
    "val_annotation_dir = r'E:\\CDUT\\English\\7 Term\\Project\\model\\archive\\dataset_2\\annotations'\n",
    "\n",
    "additional_image_dir = r'E:\\CDUT\\English\\7 Term\\Project\\model\\archive\\extra'\n",
    "additional_val_image_dir = r'E:\\CDUT\\English\\7 Term\\Project\\model\\archive\\extra_val'\n",
    "\n",
    "IMG_HEIGHT = 512\n",
    "IMG_WIDTH = 640\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def preprocess_to_grayscale(image_dir, target_size):\n",
    "    processed_images = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            img = load_img(img_path, target_size=target_size).convert('L')\n",
    "            img_array = img_to_array(img) / 255.0\n",
    "            img_array = np.repeat(img_array, 3, axis=-1)\n",
    "            processed_images.append(img_array)\n",
    "    return np.array(processed_images)\n",
    "\n",
    "def preprocess_additional_defected_images(image_dir, target_size):\n",
    "    processed_images = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            img = load_img(img_path, target_size=target_size).convert('L')  # 转为灰度\n",
    "            img_array = img_to_array(img) / 255.0  # 归一化\n",
    "            img_array = np.repeat(img_array, 3, axis=-1)  # 转为 3 通道\n",
    "            processed_images.append(img_array)\n",
    "            labels.append(1)  # 缺陷图片标签为 1\n",
    "\n",
    "    return np.array(processed_images), np.array(labels)\n",
    "\n",
    "def load_dataset_with_corners(image_dir, annotation_dir):\n",
    "    labels = []\n",
    "    corners_list = []\n",
    "    true_count = 0  # 计数 defected_module=true 的图片\n",
    "    total_images = 0  # 总图片数\n",
    "\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            total_images += 1\n",
    "            annotation_path = os.path.join(annotation_dir, filename.replace('.jpg', '.json').replace('.png', '.json'))\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                annotation = json.load(f)\n",
    "                instance_data = annotation[\"instances\"]\n",
    "\n",
    "                image_corners = []\n",
    "                image_labels = []\n",
    "                for instance in instance_data:\n",
    "                    corners = instance[\"corners\"]\n",
    "                    defected = instance[\"defected_module\"]\n",
    "                    if defected:\n",
    "                        true_count += 1\n",
    "                    image_corners.append([(corner['x'], corner['y']) for corner in corners])\n",
    "                    image_labels.append(1 if defected else 0)\n",
    "\n",
    "                corners_list.append(image_corners)\n",
    "                labels.append(image_labels)\n",
    "\n",
    "    return corners_list, labels, true_count, total_images\n",
    "\n",
    "# 加载和预处理数据集\n",
    "train_images_gray = preprocess_to_grayscale(train_image_dir, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "train_corners, train_labels, train_true_count, train_total_images = load_dataset_with_corners(train_image_dir, train_annotation_dir)\n",
    "\n",
    "val_images_gray = preprocess_to_grayscale(val_image_dir, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "val_corners, val_labels, val_true_count, val_total_images = load_dataset_with_corners(val_image_dir, val_annotation_dir)\n",
    "\n",
    "# 处理额外的缺陷图片\n",
    "additional_images, additional_labels = preprocess_additional_defected_images(additional_image_dir, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "additional_val_images, additional_val_labels = preprocess_additional_defected_images(additional_val_image_dir, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "\n",
    "# 合并数据\n",
    "train_images_combined = np.concatenate((train_images_gray, additional_images), axis=0)\n",
    "train_labels_binary = np.array([1 if any(label) else 0 for label in train_labels])\n",
    "train_labels_combined = np.concatenate((train_labels_binary, additional_labels), axis=0)\n",
    "\n",
    "# 验证集标签处理\n",
    "val_images_combined = np.concatenate((val_images_gray, additional_val_images), axis=0)\n",
    "val_labels_binary = np.array([1 if any(label) else 0 for label in val_labels])\n",
    "val_labels_combined = np.concatenate((val_labels_binary, additional_val_labels), axis=0)\n",
    "\n",
    "# 更新训练集计数\n",
    "num_defected = np.sum(train_labels_combined == 1)  # 缺陷图片数量\n",
    "num_non_defected = np.sum(train_labels_combined == 0)  # 非缺陷图片数量\n",
    "\n",
    "print(f\"Updated Training Dataset:\")\n",
    "print(f\"Total Training Images: {train_images_combined.shape[0]}\")\n",
    "print(f\"Defected Images: {num_defected}\")\n",
    "print(f\"Non-Defected Images: {num_non_defected}\")\n",
    "\n",
    "print(f\"Combined Training Images Shape: {train_images_combined.shape}\")\n",
    "print(f\"Combined Training Labels Shape: {train_labels_combined.shape}\")\n",
    "print(f\"Training Labels Distribution: {np.bincount(train_labels_combined)}\")\n",
    "\n",
    "# 标签分布可视化\n",
    "def plot_label_distribution(train_labels, val_labels):\n",
    "    categories = ['Defected', 'Non-Defected']\n",
    "\n",
    "    train_dist = [np.sum(train_labels == 1), np.sum(train_labels == 0)]\n",
    "    val_dist = [np.sum(val_labels == 1), np.sum(val_labels == 0)]\n",
    "\n",
    "    x = ['Train Set', 'Validation Set']\n",
    "    defected = [train_dist[0], val_dist[0]]\n",
    "    non_defected = [train_dist[1], val_dist[1]]\n",
    "\n",
    "    x_pos = np.arange(len(x))\n",
    "    width = 0.2\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(x_pos - width / 2, defected, width, label='Defected')\n",
    "    ax.bar(x_pos + width / 2, non_defected, width, label='Non-Defected')\n",
    "\n",
    "    ax.set_xlabel('Dataset')\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.set_title('Label Distribution by Dataset')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(x)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_label_distribution(train_labels_combined, val_labels_combined)\n",
    "\n",
    "# 模型构建\n",
    "def build_model():\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    for layer in base_model.layers[:-50]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(train_images_combined)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels_combined),\n",
    "    y=train_labels_combined\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    datagen.flow(train_images_combined, train_labels_combined, batch_size=BATCH_SIZE),\n",
    "    validation_data=(val_images_combined, val_labels_combined),\n",
    "    epochs=100,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(val_images_combined, val_labels_combined, batch_size=BATCH_SIZE)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n",
    "\n",
    "def plot_training(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training(history)\n",
    "\n",
    "predictions = model.predict(val_images_combined)\n",
    "print(f\"Predictions: {predictions[:10].flatten()}\")\n",
    "print(f\"True Labels: {val_labels_combined[:10]}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(train_images_combined[120])\n",
    "plt.title(\"Processed Training Image\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(val_images_combined[12])\n",
    "plt.title(\"Processed Testing Image\")\n",
    "plt.show()\n",
    "\n",
    "# 获取嵌套的 InceptionV3 模型\n",
    "inception_v3 = model.get_layer(\"inception_v3\")\n",
    "\n",
    "# 打印 InceptionV3 内部所有层名称\n",
    "for layer in inception_v3.layers:\n",
    "    print(layer.name)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "import seaborn as sns\n",
    "\n",
    "# 预测标签\n",
    "y_pred = model.predict(val_images_combined)\n",
    "y_pred_binary = (y_pred.flatten() > 0.5).astype(int)\n",
    "\n",
    "# 混淆矩阵\n",
    "cm = confusion_matrix(val_labels_combined, y_pred_binary)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "# 可视化混淆矩阵\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Defected\", \"Defected\"], yticklabels=[\"Non-Defected\", \"Defected\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ROC 曲线和 AUC\n",
    "fpr, tpr, _ = roc_curve(val_labels_combined, y_pred.flatten())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall 曲线和 F1 分数\n",
    "precision, recall, _ = precision_recall_curve(val_labels_combined, y_pred.flatten())\n",
    "pr_auc = auc(recall, precision)\n",
    "f1 = f1_score(val_labels_combined, y_pred_binary)\n",
    "\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f\"PR Curve (AUC = {pr_auc:.4f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 分类报告\n",
    "report = classification_report(val_labels_combined, y_pred_binary, target_names=[\"Non-Defected\", \"Defected\"])\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def generate_grad_cam(model, img_array, target_layer_name, class_idx=None):\n",
    "    # 获取目标层\n",
    "    target_layer = model.get_layer(target_layer_name)\n",
    "    grad_model = Model(inputs=model.input, outputs=[target_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "\n",
    "        # 检查 predictions 的形状\n",
    "        print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "        # 如果 class_idx 为 None，选择最大预测值对应的索引\n",
    "        if class_idx is None:\n",
    "            class_idx = tf.argmax(predictions, axis=-1)  # 获取最大预测值的类别索引\n",
    "            class_idx = tf.reduce_max(class_idx)  # 确保 class_idx 是标量\n",
    "            print(f\"Selected class_idx: {class_idx.numpy()}\")\n",
    "\n",
    "        # 确保 class_idx 是标量\n",
    "        class_idx = tf.cast(class_idx, tf.int32)\n",
    "\n",
    "        # 计算 loss\n",
    "        loss = tf.reduce_mean(predictions[..., class_idx])  # 确保 predictions 是 [batch_size, height, width, num_classes] 格式\n",
    "\n",
    "    # 计算梯度\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # 生成热力图\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def display_grad_cam(img_path, heatmap, output_size=(640, 512), alpha=0.6):\n",
    "    img = load_img(img_path)\n",
    "    img = img.resize(output_size)\n",
    "    img = img_to_array(img).astype(\"uint8\")\n",
    "    \n",
    "    # 确保热力图的大小和通道数与输入图像匹配\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.resize(heatmap, (output_size[0], output_size[1]))  # 调整热力图大小\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # 确保输入图像和热力图的通道数匹配\n",
    "    if img.shape[-1] == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # 检查图像和热力图的大小和通道数\n",
    "    print(f\"Image shape: {img.shape}\")\n",
    "    print(f\"Heatmap shape: {heatmap.shape}\")\n",
    "    \n",
    "    # 调整图像和热力图的大小和通道数\n",
    "    if img.shape[:2] != heatmap.shape[:2]:\n",
    "        heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    if img.shape[-1] != heatmap.shape[-1]:\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    overlay = cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(overlay)\n",
    "    plt.colorbar(plt.cm.ScalarMappable(cmap='jet'), orientation='vertical', fraction=0.046, pad=0.04)\n",
    "    plt.show()\n",
    "    \n",
    "# 预处理输入图像\n",
    "def preprocess_image(img_path, target_size=(512, 640)):\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "\n",
    "# 模型加载\n",
    "model = tf.keras.models.load_model('./solar_cell_model_with_inceptionv3.h5')  # 替换为你的模型路径\n",
    "\n",
    "# 提取嵌套的 InceptionV3 模型\n",
    "inception_model = model.get_layer(\"inception_v3\")\n",
    "\n",
    "# 目标层名称设置为 InceptionV3 的最后一个卷积层\n",
    "target_layer_name = \"mixed10\"\n",
    "\n",
    "# 测试图像路径\n",
    "img_path = r'E:\\CDUT\\English\\7 Term\\Project\\model\\archive\\dataset\\images\\041R.jpg'  # 替换为你的测试图像路径\n",
    "img_array = preprocess_image(img_path, target_size=(512, 640))\n",
    "\n",
    "# 生成 Grad-CAM\n",
    "heatmap = generate_grad_cam(inception_model, img_array, target_layer_name)\n",
    "\n",
    "# 显示结果\n",
    "display_grad_cam(img_path, heatmap)\n",
    "\n",
    "from numba import cuda\n",
    "\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
